apiVersion: v1
kind: Pod
metadata:
  name: ai-stack
  labels:
    app: ai-stack
  annotations:
    io.containers.autoupdate: "registry"
    # Essential for AMD GPU access in Podman
    io.podman.annotations.device/ollama: "/dev/kfd,/dev/dri"
spec:
  containers:
  - name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    env:
      - name: HOSTNAME
        value: open-webui
      - name: OLLAMA_BASE_URL
        value: "http://127.0.0.1:11434"
    ports:
    - containerPort: 8080
      hostIP: 127.0.0.1
      hostPort: 3000
    volumeMounts:
    - mountPath: /app/backend/data
      name: open-webui-pvc
    resources:
      limits:
        memory: 4Gi

  - name: ollama
    image: docker.io/ollama/ollama:rocm
    args:
    - serve
    securityContext:
      privileged: true
    env:
    - name: HOME
      value: /root
    - name: HOSTNAME
      value: ollama
    ports:
    - containerPort: 11434
      hostIP: 127.0.0.1
      hostPort: 11434
    volumeMounts:
    - mountPath: /root/.ollama
      name: ollama-pvc
    - mountPath: /dev/shm
      name: shm
    resources:
      limits:
        memory: 16Gi # Adjust based on your available System RAM/VRAM
  volumes:
  - name: open-webui-pvc
    persistentVolumeClaim:
      claimName: open-webui-data
  - name: ollama-pvc
    persistentVolumeClaim:
      claimName: ollama-data
  - name: shm
    hostPath:
      path: /dev/shm
      type: Directory
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: open-webui-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi # Increased for multiple LLM models
